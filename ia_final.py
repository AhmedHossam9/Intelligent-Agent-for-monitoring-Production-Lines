# -*- coding: utf-8 -*-
"""IA_final.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Rv2BPOOeTJsXMFW5s5UjRLW75AN6ycTN

# ***`[CNN_LTSM]`***
"""

import os
import torch
import torchvision.transforms as transforms
import torch.nn as nn
import torch.optim as optim
import torchvision.models as models
from torch.utils.data import DataLoader, Dataset
import cv2
import numpy as np
from tqdm import tqdm
from sklearn.metrics import accuracy_score
from sklearn.model_selection import train_test_split

class VideoFrameDataset(Dataset):
    def __init__(self, root_dir, transform=None, sequence_length=30, overlap=0):
        self.root_dir = root_dir
        self.transform = transform
        self.sequence_length = sequence_length
        self.overlap = overlap
        self.classes = sorted(os.listdir(root_dir))
        self.data = []

        for label in self.classes:
            class_path = os.path.join(root_dir, label)
            for video_folder in os.listdir(class_path):
                video_path = os.path.join(class_path, video_folder)
                frames = sorted(os.listdir(video_path))
                num_frames = len(frames)
                step = sequence_length - overlap
                for start in range(0, num_frames - sequence_length + 1, step):
                    self.data.append((video_path, frames[start:start + sequence_length], self.classes.index(label)))

    def __len__(self):
        return len(self.data)

    def __getitem__(self, idx):
        video_path, frames, label = self.data[idx]
        frame_sequence = []

        print(f"Loading sequence from: {video_path}, Label: {label}")

        for frame_file in frames:
            frame = cv2.imread(os.path.join(video_path, frame_file))
            frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
            if self.transform:
                frame = self.transform(frame)
            frame_sequence.append(frame)

        frame_sequence = torch.stack(frame_sequence)
        print(f"Loaded sequence length: {len(frame_sequence)}")
        return frame_sequence, label

# Data augmentation and normalization
transform = transforms.Compose([
    transforms.ToPILImage(),
    transforms.Resize((128, 128)),
    transforms.RandomHorizontalFlip(),
    transforms.RandomRotation(10),
    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.2),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
])

# Create the dataset with overlapping sequences
dataset = VideoFrameDataset(root_dir='/content/drive/Shareddrives/IA/data1/train', transform=transform, sequence_length=30, overlap=10)

# Split the dataset into training and validation sets
train_indices, val_indices = train_test_split(list(range(len(dataset))), test_size=0.2, random_state=42)

train_sampler = torch.utils.data.SubsetRandomSampler(train_indices)
val_sampler = torch.utils.data.SubsetRandomSampler(val_indices)

train_loader = DataLoader(dataset, batch_size=4, sampler=train_sampler, num_workers=2)
val_loader = DataLoader(dataset, batch_size=4, sampler=val_sampler, num_workers=2)


class ResNetLSTM(nn.Module):
    def __init__(self, num_classes=3, hidden_size=256, num_layers=3):
        super(ResNetLSTM, self).__init__()
        resnet = models.resnet18(weights=models.ResNet18_Weights.IMAGENET1K_V1)
        self.cnn = nn.Sequential(*list(resnet.children())[:-2])
        self.lstm = nn.LSTM(512 * 4 * 4, hidden_size, num_layers, batch_first=True, dropout=0.5)
        self.fc = nn.Linear(hidden_size, num_classes)
        self.dropout = nn.Dropout(p=0.5)

    def forward(self, x):
        batch_size, seq_len, c, h, w = x.size()
        c_in = x.view(batch_size * seq_len, c, h, w)
        c_out = self.cnn(c_in)
        c_out = c_out.view(batch_size, seq_len, -1)
        r_out, _ = self.lstm(c_out)
        r_out = self.dropout(r_out)
        output = self.fc(r_out[:, -1, :])
        return output

model = ResNetLSTM().cuda()

# Training setup remains the same
criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr=0.0001, weight_decay=1e-5)
scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.1)

# Training and validation loop remains the same
num_epochs = 3
patience = 5
best_val_loss = float('inf')
early_stop_counter = 0

for epoch in range(num_epochs):
    model.train()
    running_loss = 0.0
    running_corrects = 0
    total_samples = 0

    for frames, labels in tqdm(train_loader, desc=f"Epoch {epoch+1}/{num_epochs}"):
        frames, labels = frames.cuda(), labels.cuda()
        optimizer.zero_grad()
        outputs = model(frames)
        loss = criterion(outputs, labels)
        loss.backward()
        optimizer.step()
        running_loss += loss.item()

        _, preds = torch.max(outputs, 1)
        running_corrects += torch.sum(preds == labels.data)
        total_samples += labels.size(0)

    train_loss = running_loss / len(train_loader)
    train_accuracy = running_corrects.double() / total_samples
    scheduler.step()

    # Validation phase
    model.eval()
    val_loss = 0.0
    val_corrects = 0
    val_samples = 0
    all_labels = []
    all_preds = []

    with torch.no_grad():
        for frames, labels in val_loader:
            frames, labels = frames.cuda(), labels.cuda()
            outputs = model(frames)
            loss = criterion(outputs, labels)
            val_loss += loss.item()
            _, preds = torch.max(outputs, 1)
            val_corrects += torch.sum(preds == labels.data)
            val_samples += labels.size(0)
            all_labels.extend(labels.cpu().numpy())
            all_preds.extend(preds.cpu().numpy())

    val_loss /= len(val_loader)
    val_accuracy = accuracy_score(all_labels, all_preds)
    print(f"Epoch [{epoch+1}/{num_epochs}], Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}, Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f}")

    # Early stopping
    if val_loss < best_val_loss:
        best_val_loss = val_loss
        early_stop_counter = 0
        torch.save(model.state_dict(), '/content/drive/Shareddrives/IA/cnn_lstm_model.pth')
    else:
        early_stop_counter += 1
        if early_stop_counter >= patience:
            print("Early stopping triggered")
            break

import torch
import torchvision.transforms as transforms
import torch.nn as nn
import cv2
import numpy as np

class EnhancedCNNLSTM(nn.Module):
    def __init__(self, num_classes=3, hidden_size=256, num_layers=3):
        super(EnhancedCNNLSTM, self).__init__()
        self.cnn = nn.Sequential(
            nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1),
            nn.BatchNorm2d(64),
            nn.ReLU(),
            nn.MaxPool2d(kernel_size=2, stride=2),
            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),
            nn.BatchNorm2d(128),
            nn.ReLU(),
            nn.MaxPool2d(kernel_size=2, stride=2),
            nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1),
            nn.BatchNorm2d(256),
            nn.ReLU(),
            nn.MaxPool2d(kernel_size=2, stride=2)
        )
        self.lstm = nn.LSTM(256 * 16 * 16, hidden_size, num_layers, batch_first=True, dropout=0.5)
        self.fc = nn.Linear(hidden_size, num_classes)
        self.dropout = nn.Dropout(p=0.5)

    def forward(self, x):
        batch_size, seq_len, c, h, w = x.size()
        c_in = x.view(batch_size * seq_len, c, h, w)
        c_out = self.cnn(c_in)
        r_in = c_out.view(batch_size, seq_len, -1)
        r_out, _ = self.lstm(r_in)
        r_out = self.dropout(r_out)
        output = self.fc(r_out[:, -1, :])
        return output

# Load the trained model
model_path = '/content/drive/Shareddrives/IA/cnn_lstm_model.pth'
model = EnhancedCNNLSTM()
model.load_state_dict(torch.load(model_path))
model = model.cuda()
model.eval()

# Data augmentation and normalization for inference
transform = transforms.Compose([
    transforms.ToPILImage(),
    transforms.Resize((128, 128)),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
])

def preprocess_frames(frames):
    processed_frames = []
    for frame in frames:
        frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
        frame = transform(frame)
        processed_frames.append(frame)
    return torch.stack(processed_frames)

def classify_video(video_path, sequence_length=30):
    cap = cv2.VideoCapture(video_path)
    frames = []
    predictions = []

    while True:
        ret, frame = cap.read()
        if not ret:
            break
        frames.append(frame)

        if len(frames) == sequence_length:
            input_frames = preprocess_frames(frames)
            input_frames = input_frames.unsqueeze(0).cuda()  # Add batch dimension
            with torch.no_grad():
                output = model(input_frames)
                _, pred = torch.max(output, 1)
                predictions.append(pred.item())
            frames = frames[1:]  # Slide the window

    cap.release()
    return predictions

# Example usage
video_path = '/content/drive/Shareddrives/IA/test1.mp4'
predictions = classify_video(video_path)

# Mapping class indices to class names (assumes classes are sorted alphabetically in your dataset)
class_names = ['failure_type_a', 'failure_type_b', 'success']
predicted_classes = [class_names[p] for p in predictions]

print(predicted_classes)

from google.colab import drive
drive.mount('/content/drive')

"""# ***`3D CNN`***

> model needs VRAM cuda out of memory


"""

import os
import torch
from torch.utils.data import Dataset, DataLoader
from torchvision import transforms
from PIL import Image
import torch.nn as nn
import torch.optim as optim

class VideoFrameDataset(Dataset):
    def __init__(self, root_dir, transform=None):
        self.root_dir = root_dir
        self.transform = transform
        self.classes = sorted(os.listdir(root_dir))
        self.frames = []
        for label in self.classes:
            class_path = os.path.join(root_dir, label)
            for video_folder in os.listdir(class_path):
                video_frames = sorted([os.path.join(class_path, video_folder, frame)
                                       for frame in os.listdir(os.path.join(class_path, video_folder))])
                if len(video_frames) >= 30:
                    self.frames.append((video_frames[:30], self.classes.index(label)))

    def __len__(self):
        return len(self.frames)

    def __getitem__(self, idx):
        frames, label = self.frames[idx]
        images = [Image.open(frame).convert('RGB') for frame in frames]
        if self.transform:
            images = [self.transform(image) for image in images]
        images = torch.stack(images)  # Stack frames: [sequence_length, channels, height, width]
        images = images.permute(1, 0, 2, 3)  # Permute to: [channels, sequence_length, height, width]
        return images, label

# Ensure that your transform includes necessary resizing and tensor conversion
transform = transforms.Compose([
    transforms.Resize((128, 128)),
    transforms.ToTensor(),
])

dataset = VideoFrameDataset('/content/drive/Shareddrives/IA/data1/train', transform=transform)
dataloader = DataLoader(dataset, batch_size=2, shuffle=True)

class Simple3DCNN(nn.Module):
    def __init__(self):
        super(Simple3DCNN, self).__init__()
        self.conv1 = nn.Conv3d(3, 64, kernel_size=(3, 3, 3), padding=1)
        self.pool = nn.MaxPool3d((1, 2, 2))
        # Assuming one pool operation reducing spatial dimensions by a factor of 2:
        # Output dimensions from conv1: [batch, 64, 30, 64, 64] (depth is unchanged, H and W halved)
        # Calculate flattened size:
        num_features = 64 * 30 * 64 * 64  # Channels * depth * height * width
        self.fc1 = nn.Linear(num_features, 100)  # Adjust this line to match calculated num_features
        self.fc2 = nn.Linear(100, 3)

    def forward(self, x):
        x = self.pool(torch.relu(self.conv1(x)))
        x = x.view(x.size(0), -1)  # Flatten keeping the batch size intact
        x = torch.relu(self.fc1(x))
        x = self.fc2(x)
        return x

# Now let's instantiate the model and move it to GPU
model = Simple3DCNN().cuda()

criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr=0.001)
import torch

num_epochs = 20
for epoch in range(num_epochs):
    for inputs, labels in dataloader:
        # Ensure inputs are in the correct shape if not already done in the dataset class
        # Shape of inputs expected: [batch_size, sequence_length, channels, height, width]
        # If your dataloader doesn't already format the inputs correctly, you might need:
        # inputs = inputs.permute(0, 2, 1, 3, 4) # Only if necessary

        # Move tensors to the GPU
        inputs = inputs.cuda()
        labels = labels.cuda()

        # Zero the parameter gradients
        optimizer.zero_grad()

        # Forward + backward + optimize
        print("Input shape:", inputs.shape)
        outputs = model(inputs)
        print("Output shape:", outputs.shape)
        loss = criterion(outputs, labels)
        loss.backward()
        optimizer.step()

    print(f'Epoch {epoch+1}, Loss: {loss.item()}')


# Save the model
torch.save(model.state_dict(), '/content/drive/Shareddrives/IA/classifier.pth')

"""# ***`RL Model`***

> model needs RAM session dies


"""

import gym
from gym import spaces
import numpy as np
import os
from PIL import Image
import torch
import torch.nn as nn
import torch.optim as optim
import random
from collections import deque
from torchvision import transforms


class FrameClassificationEnv(gym.Env):
    def __init__(self, data_path, transform=None):
        super(FrameClassificationEnv, self).__init__()
        self.data_path = data_path
        self.transform = transform
        self.action_space = spaces.Discrete(3)  # Three possible classes
        self.observation_space = spaces.Box(low=0, high=255, shape=(30, 224, 224, 3), dtype=np.uint8)
        self.load_data()

    def load_data(self):
        self.sequences = []
        self.labels = []
        label_map = {'failure_type_a': 0, 'failure_type_b': 1, 'success': 2}
        for label in os.listdir(self.data_path):
            class_path = os.path.join(self.data_path, label)
            for video_folder in os.listdir(class_path):
                video_path = os.path.join(class_path, video_folder)
                frames = sorted(os.listdir(video_path))
                for i in range(len(frames) - 29):
                    sequence_paths = frames[i:i+30]
                    sequence = [self.transform(Image.open(os.path.join(video_path, frame))) for frame in sequence_paths]
                    self.sequences.append(np.stack(sequence, axis=0))
                    self.labels.append(label_map[label])

    def step(self, action):
        reward = 1 if action == self.current_label else 0
        self.current_index += 1
        done = self.current_index == len(self.sequences)
        next_state = self.sequences[self.current_index] if not done else None
        return next_state, reward, done, {}

    def reset(self):
        self.current_index = 0
        self.current_label = self.labels[self.current_index]
        return self.sequences[self.current_index]

    def render(self, mode='human'):
        pass  # Optional for visualization



class ConvLSTMClassifier(nn.Module):
    def __init__(self):
        super(ConvLSTMClassifier, self).__init__()
        self.conv = nn.Sequential(
            nn.Conv2d(3, 16, kernel_size=3, padding=1),
            nn.ReLU(),
            nn.MaxPool2d(2),
            nn.Conv2d(16, 32, kernel_size=3, padding=1),
            nn.ReLU(),
            nn.MaxPool2d(2),
        )
        self.lstm = nn.LSTM(32 * 56 * 56, 128, batch_first=True)
        self.fc = nn.Linear(128, 3)  # Output a Q-value for each class

    def forward(self, x):
        batch_size, timesteps, C, H, W = x.size()
        c_in = x.view(batch_size * timesteps, C, H, W)
        c_out = self.conv(c_in)
        r_in = c_out.view(batch_size, timesteps, -1)
        r_out, _ = self.lstm(r_in)
        r_out = r_out[:, -1, :]
        output = self.fc(r_out)
        return output


class ReplayBuffer:
    def __init__(self, capacity):
        self.buffer = deque(maxlen=capacity)

    def push(self, state, action, reward, next_state, done):
        self.buffer.append((state, action, reward, next_state, done))

    def sample(self, batch_size):
        return random.sample(self.buffer, batch_size)

    def __len__(self):
        return len(self.buffer)

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model = ConvLSTMClassifier().to(device)
optimizer = optim.Adam(model.parameters(), lr=0.001)
criterion = nn.MSELoss()
replay_buffer = ReplayBuffer(10000)
batch_size = 32
gamma = 0.99  # discount factor

def compute_loss(batch):
    states, actions, rewards, next_states, dones = zip(*batch)
    states = torch.stack(states).to(device)
    actions = torch.tensor(actions).to(device)
    rewards = torch.tensor(rewards).to(device)
    next_states = torch.stack(next_states).to(device)
    dones = torch.tensor(dones).to(device)

    current_q = model(states).gather(1, actions.unsqueeze(1)).squeeze(1)
    max_next_q = model(next_states).max(1)[0]
    expected_q = rewards + gamma * max_next_q * (1 - dones)
    loss = criterion(current_q, expected_q)
    return loss

def train_dqn(epochs):
    env = FrameClassificationEnv(data_path, transform=transform)
    state = env.reset()
    total_reward = 0

    for epoch in range(epochs):
        action = model(torch.tensor(state).unsqueeze(0).to(device)).max(1)[1].item()
        next_state, reward, done, _ = env.step(action)
        replay_buffer.push(state, action, reward, next_state, done)
        state = next_state
        total_reward += reward

        if len(replay_buffer) > batch_size:
            batch = replay_buffer.sample(batch_size)
            loss = compute_loss(batch)
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()

        if done:
            state = env.reset()
            print(f"Epoch {epoch+1}: Total Reward: {total_reward}")
            total_reward = 0

    # Save the model
    torch.save(model.state_dict(), '/content/drive/Shareddrives/IA/video_classifierRL.pth')

# Define the path and transformations
data_path = '/content/drive/Shareddrives/IA/data1/train'
transform = transforms.Compose([
    transforms.Resize((224, 224)),
    transforms.ToTensor(),
])

train_dqn(100)

import os
import torch
from torch.utils.data import Dataset, DataLoader
from torchvision import transforms, datasets
from PIL import Image

class VideoFrameDataset(Dataset):
    def __init__(self, root_dir, transform=None):
        self.root_dir = root_dir
        self.transform = transform
        self.samples = []

        # Scan through each folder to collect sequences
        classes = sorted(os.listdir(root_dir))  # class folders
        for idx, cls in enumerate(classes):
            class_path = os.path.join(root_dir, cls)
            videos = sorted(os.listdir(class_path))
            for video in videos:
                video_path = os.path.join(class_path, video)
                frames = sorted(os.listdir(video_path))
                if len(frames) >= 30:  # Ensure there are at least 30 frames
                    self.samples.append((video_path, idx))

    def __len__(self):
        return len(self.samples)

    def __getitem__(self, idx):
        video_path, label = self.samples[idx]
        frames = sorted([os.path.join(video_path, frame) for frame in os.listdir(video_path)][:30])

        video_tensor = torch.stack([self.transform(Image.open(frame)) for frame in frames])
        video_tensor = video_tensor.permute(1, 0, 2, 3)  # Change dimension order to C, D, H, W
        return video_tensor, label
# Data transformation and augmentation
transform = transforms.Compose([
    transforms.Resize((128, 128)),
    transforms.ToTensor(),
])

# Dataset loader
train_dataset = VideoFrameDataset('/content/drive/Shareddrives/IA/data1/train', transform=transform)
train_loader = DataLoader(train_dataset, batch_size=2, shuffle=True)
import torch.nn as nn
import torch.nn.functional as F

class Simple3DCNN(nn.Module):
    def __init__(self):
        super(Simple3DCNN, self).__init__()
        self.conv1 = nn.Conv3d(3, 64, kernel_size=(3, 3, 3), padding=1)
        self.pool = nn.MaxPool3d((1, 2, 2))
        self.conv2 = nn.Conv3d(64, 128, kernel_size=(3, 3, 3), padding=1)
        self.fc1 = nn.Linear(128 * 30 * 32 * 32, 512)
        self.fc2 = nn.Linear(512, 3)

    def forward(self, x):
        x = self.pool(F.relu(self.conv1(x)))  # First conv and pool
        x = self.pool(F.relu(self.conv2(x)))  # Second conv and pool
        print("Shape before flattening:", x.shape)  # Add this line to print shape
        x = x.view(-1, 128 * 30 * 32 * 32)
        x = F.relu(self.fc1(x))
        x = self.fc2(x)
        return x

model = Simple3DCNN()
model.cuda()  # Move model to CUDA
import torch.optim as optim

criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr=0.001)
# Training loop
for epoch in range(10):  # number of epochs
    for inputs, labels in train_loader:
        print(inputs.shape)
        inputs, labels = inputs.cuda(), labels.cuda()  # Move inputs and labels to CUDA
        optimizer.zero_grad()
        outputs = model(inputs)
        loss = criterion(outputs, labels)
        loss.backward()
        optimizer.step()
    print(f'Epoch {epoch+1}, Loss: {loss.item()}')